# Create a Neural_Network class
class Neural_Network(object):    
    def __init__(self,inputSize = 2,outputSize = 1 ):        
        # size of layers
        self.inputSize = inputSize
        self.outputSize = outputSize    
        #weights
        self.W1 = np.random.random(size = (inputSize+1, outputSize)) # randomly initialize W1 using random function of numpy
        # size of the wieght will be (inputSize +1, outputSize) that +1 is for bias 
        
         

    def feedforward(self, X):
        
        self.b1 = np.ones([X.shape[0],1], dtype = float)
        self.a = np.dot(X ,self.W1) + self.b1
        #print ("a: ", a)
        z = self.sigmoid(self.a)
        #print ("b: ", b)
        #forward propagation through our network
        # dot product of X (input) and set of weights
        # apply activation function (i.e. sigmoid)    
        return z # return your answer with as a final output of the network

    def sigmoid(self, s):
        # activation function
        sig = 1/(1 + np.exp(-s))
        return sig # apply sigmoid function on s and return it's value

    def sigmoid_derivative(self, s):
        #sigDer = np.exp(-s)/(1 + np.exp(-s))**2
        #derivative of sigmoid
        sigDer = self.sigmoid(s)* (1- self.sigmoid(s))
        print ("sigDerShape", sigDer.shape)
        return sigDer # apply derivative of sigmoid on s and return it's value 

    def backwardpropagate(self,X, Y, y_pred, lr):
        # backward propagate through the network
        
        error = y_pred - Y
        #print ("ErrorDer: ", errorDer)
        #dE = self.sigmoid_derivative()
        #print ("X shape: ", X.shape)
        self.W1 -= lr * np.dot(X.transpose(), error) 
 
        
    
    def crossentropy(self, Y, Y_pred):
        # compute error based on crossentropy loss 
        crossEnt = - sum([ (Y[i] * np.log(Y_pred[i])) for i in range(len(Y_pred))])
        #num = Y.shape[0]
        #res = Y_pred - Y
        #crossEnt = res/num
        #print ("cross entrophy error: ", crossEnt)
        return crossEnt #error
    

    def train(self, trainX, trainY,epochs = 10, learningRate = 0.001, plot_err = True ,validationX = validX, validationY = validY):
        # feed forward trainX and trainY and recivce predicted value
        # backpropagation with trainX, trainY, predicted value and learning rate.
        # if validationX and validationY are not null than show validation accuracy and error of the model.
        # plot error of the model if plot_err is true
        self.trainLoss = []
        for i in range(epochs):
            #y_Pred = self.feedforward(trainX)
            y_pred = self.feedforward(trainX)
            self.trainLoss.append (self.crossentropy(trainY, y_pred))
            self.backwardpropagate(trainX, trainY, y_pred, learningRate)
            #print (np.shape(trainLoss))

    def predict(self, testX):
        # predict the value of testX
        pred = self.feedforward(testX)
        pred = [1 if x>=0.5 else 0 for x in pred]
        
    
    def accuracy(self, testX, testY):
        # predict the value of trainX
        # compare it with testY
        # compute accuracy, print it and show in the form of picture

        pred = self.feedforward(testX)
        for i in range(pred.shape[0]):
            if (pred.item(i) < 0.5):
                pred[i]=0
            else:
                pred[i]=1
        #pred = [1 if x>=0.5 else 0 for x in pred]
        #comp = [comp.append(((pred[i] - testY[i])**2)/2) for i in range(pred)]
        #comp = np.square(np.subtract(testY,pred)).mean() 
        var = 0
        acc =0
        print ("pred: ", pred)
        print ("testY: ", testY)
        #bool_ar = (pred == testY).all()
        #print (bool_ar)
        var = sum((bool(x) for x in pred))
        print ("Sum in acc", var)
        
                  
        #for i in range(testX.shape[0]):
           # if(np.equal(pred[i],testY[i])):
            #if ((pred[i][0] == testY[i][0]).all()):
                #var=var+1
        acc = 100*(var)/testY.shape[0]
        print ("Accuracy", acc)
        #acc =1    
        
        return acc # return accuracy    
        
    def saveModel(self,name):
        # save your trained model, it is your interpretation how, which and what data you store
        # which you will use later for prediction
        joblib.dump(self,name)

        
    def loadModel(self,name):
        # load your trained model, load exactly how you stored it.
        joblib.load(self,name)
        
    def plotErr(self):
        plt.plot(self.trainLoss, label = 'Training Error')
        #plt.plot(historyTest)
        plt.title('Train loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(loc='upper right')
        plt.show()
  
###########

#prepare training, testing and validation data and store it in the mentioned variable e.g. trainX, trainY, testX, testY, validX and validY
        
model = Neural_Network(2,1)
# try different combinations of epochs and learning rate
model.train(trainX, trainY, epochs = 150, learningRate = 0.001, validationX = validX, validationY = validY)

print (model.accuracy(testX,testY))
model.plotErr()    
